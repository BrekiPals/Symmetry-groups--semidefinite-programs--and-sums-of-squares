\documentclass[]{article}
\usepackage{MT}

\addbibresource{references.bib}
\begin{document}

\title{Symmetry groups, semidefinite programs, and sums of squares}
\author{Breki PÃ¡lsson and Simon Wegan}
\date{20. April 2023}

\maketitle

\section*{Introduction}
We will look at a fundamental problem in real algebraic geometry i.e. the existence and computation of a
representation of a multivariate polynomial as a sum of squares (SOS). In other words, the
question of finding $p_i \in \R[x], i = 1, \cdots, N$ such that

\[
    f(\mathbf{x}) = \sum_{i=1}^{N}(p_i(\mathbf{x}))^2,
\]
where $f$ is a polynomial of degree $d$ in $n$ variables $\mathbf{x}=(x_1,\ldots,x_n)$.\\
This problem has applications in many fields of applied mathematics, such as continuous and combinatorial optimization as well as being theoretically interesting. 
One particular application is in minimization of multivariate polynomials
\[
    f^\ast=\min_{x\in\R^n}f(x),
\]
where $f$ is a polynomial in $n$ variables.\\
We can relax this problem to find 
\[
    f^{SOS}=\max_{f(\mathbf{x})-\lambda \in \Sigma}\lambda,
\]
which gives us a lower bound for $f^\ast$, and is efficiently solvable by its well-known representation as a semidefinite program (SDP).\\

The methods we present exploit symmetries in polynomials and semidefinite programming in order to get a reduction in the problem size for this latter problem. 
The main ideas are to
\begin{itemize}
    \item only consider invariant matrices in the feasible region, and to
    \item compute everything in invariants instead of the original variables.
\end{itemize}
Both of these lead to reductions in problem size.
We want to do this for two reasons: Firstly, we reduce the computational burden and secondly, smaller problem size leads to more accurate solutions may that be due to numerical conditioning or numerical errors.\\

The paper outlines the theoretical background and gives the reader examples explaining the definitions step by step in order to present an algorithm that is able to use the 
symmetric properties of a polynomial that is invariant with respect to a certain representation and produces a solution to a semidefinite program given certain constraints.\\


Our summary is divided into multiple sections: 
At first, we take a closer look at the ideas that make everything work. We do this by first focussing on a general invariant SDP, where we can make simplifications using the representation theory of finite groups. Then we consider more closely the problem of representing a polynomial as a sum of squares, where the methodology is closer to invariant theory.
Next, we present a concrete algorithm to solve for $f^{SOS}$ and obtain a certificate of the non-negativity of $f(\mathbf{x})-f^{SOS}$, complete with an example. Lastly, we summarize the possible computational savings.\\%TODO: Check if this is still up to date when we print.

\subsection*{Representation Theory}

Given a polynomial that has symmetries we want to use symmetry reduction techniques to reduce our problem to a smaller semidefinite program.

We will use representation theory to define a good notion of symmetries. In particular we will look at representations
$\sigma:G\rightarrow \mathrm{Aut}(S^N)$ that preserve $\mathcal{S}_+^N$ and are \textit{induced} by a representation $\rho:G\rightarrow \mathrm{O}(N)\subseteq\mathrm{GL}(\R^N)$, 
that is 
\begin{itemize}
    \item $\sigma(g)(\mathcal{S}_+^N)\subseteq \mathcal{S}_+^N$ for all $g\in G$, and
    \item $\sigma(g)(X) := \rho(g)^TX\rho(g)$ for all $X\in \mathcal{S} , g\in G.$
\end{itemize}

If we have an \textit{invariant} semidefinite program 
\[
    F^\ast=\min_{X\in\mathcal{L}\cap\mathcal{S}_+^N}F(X),
\]
which is to say that $F$ is $\sigma$-invariant and $\mathcal{L}$ is $\sigma$-stable, then we can always find a minimizer $X^\ast$ inside the \textit{fixed-point subspace} 
\[
    \mathcal{F} := \{X\in\mathcal{S}^N | X= \sigma(g)(X)\,\,\, \forall g \in G\}
\]

Thus we can restrict our set of feasible matrices from $\mathcal{L}\cap \mathcal{S}_+^N$ to $ X\in\mathcal{F}\cup\mathcal{L}\cap \mathcal{S}_+^N$ making the problem simpler.

With a convenient change of coordinates into a \textit{symmetry-adapted} basis every invariant matrix will be block diagonal. This comes from the fact that every linear representation of a finite group $G$ over $\R$ has a canonical decomposition as a direct sum of irreducible representations and a corresponding decomposition of $\R^N$, i.e.

\[\rho = m_1\vartheta_i\oplus m_2\vartheta_2 \oplus \cdots \oplus m_h\vartheta_h, \quad \quad \R^N = V_1\oplus V_2 \cdots \oplus V_h,.\]
together with Schur's lemma. 
On the level of vector spaces the we have a corresponding composition


The problem therefore collapses into a collection of smaller 
optimization problems, which are much easier to solve as outlined below 
\[F^\ast = \min_{X\in\mathcal{L},~X_i \in \mathcal{S}_+^{m_i} }\sum_{i=1}^{h} n_i\left<C_i,X_i\right>.\]
We end this section with a road map of reduction of complexity:
\[\min_{x\in\R^n}f(\mathbf{x}) \rightarrow \max_{f(\mathbf{x})-\lambda\in \Sigma} \lambda \rightarrow\min_{X\in\mathcal{L}\cap \mathcal{S}_+^N } \left<C,X\right> \rightarrow 
\min_{X\in\mathcal{F}\cap\mathcal{L}\cap \mathcal{S}_+^N } \left<C,X\right> \rightarrow
\min_{X\in\mathcal{L}, X_i \in \mathcal{S}_+^{m_i} }\sum_{i=1}^{h} n_i\left<C_i,X_i\right>\]
\subsection*{Invariant theory}
We will now look at some invariant theory in order to compute everything in invariant polynomials instead of the original variables.

Assume we are interested in finding the sum of squares decomposition of a polynomial $f(\textbf{x})$ of degree $2d$ in $n$
variables which is invariant with respect to a linear representation $\vartheta:G\rightarrow \mathrm{GL}(\R^n)$, i.e. $f(\textbf{x}) = f(\vartheta(g)\textbf{x})$ for all $g\in G$.
The set of all such invariant polynomials is the invariant ring, denoted by $\R[\textbf{x}]^G$. We could jump to the conclusion that if a $f\in \R[\textbf{x}]^G$ can be written as a 
sum of squares $f(\textbf{x}) =\sum_{i=1}^{h}(p_i(\textbf{x}))^2$ then the polynomials $p_i$ must also be in $\R[\mathbf{x}]^G$. Unfortunately this turns out not to be true. We can however
use a technique that allows us to couple together squares so they decompose into invariant components.

First we note that if we can write $f$ as a sum of squares then we can express it as $f(\textbf{x}) = z^TQz \left<Q,zz^T\right>$ and if we assume that $f\in \R[\mathbf{x}]^G$ is an invariant polynomial and use methods described in the previous section we can actually write it as 
\[f(\textbf{x}) = \sum_{i=1}^{h}\left<Q_i,P_i(\mathbf{x})\right>,\]
with $P_i\in (\R[\mathbf{x}]^G)^{m_i\times m_i}$
and the $Q_i$ are as always positive semidefinite.  

Furthermore we can use the so called Hironaka decomposition of the invariant ring, i.e.
\[\R[\mathbf{x}]^G = \bigoplus_{j=1}^{t}\eta_j(\textbf{x})\R[\theta_1(\textbf{x}), \cdots, \theta_n(\mathbf{x})]\] 
where $\theta_i(\mathbf{x})$, $\eta_i(\mathbf{x})$ are called primary and secondary invariants, respectively, in order to represent $f(\mathbf{x})$ uniquely as a function with the invariants as inputs. i.e.

\[\tilde{f}(\theta,\eta) = \sum_{i=1}^{h}\left<Q_i,\tilde{P}_i(\theta,\eta)\right>,\] 
with $\tilde{P_i} \in (\tilde{T})^{m_i\times m_i}$, where $\tilde{T} = \sum_{i=1}^{h}\eta_j\R[\theta]$ and the $Q_i$ are as always positive semidefinite.\\

In practice very often the group representations are so-called reflection groups. In this situation the invariant ring is 
isomorphic to a polynomial ring and the secondary invariants are not needed. 
Next we want to introduce another symmetry concept. We say that a polynomial mapping $f:\R^n\rightarrow \R^{n_i}$ with $f_i\in\R[\mathbf{x}]$ and 
\[ f(\vartheta(g)\mathbf{x}) =\vartheta_i f(\mathbf{x})\quad \forall g\in G, \]
is an $\vartheta$-$\vartheta_i$-equivariant. The set $M_i$ of $\vartheta$-$\vartheta_i$-equivariants forms is a finitely-generated module over
the invariant ring $\R[\mathbf{x}]^G$. We want to exploit the algebraic structure of the module, by writing 
\[M_i = \R[\theta]\cdot\{b_1^i,\cdots,b_{r_i}^i\},\]
where $b_{\nu}^i\in \R[\mathbf{x}]^{n_i}$ denote the elements in the module basis and $r_i$ is the rank of the module.  In addition to this we choose a special vector space basis $v_i(\mathbf{x})$ of $V_i$ determined by the module basis, such that
\[\begin{bmatrix}v_{i1}\\ \vdots \\ v_{in_i}\end{bmatrix}= \left\{1,\theta_1,\cdots, \theta_n,\theta_1^2,\cdots\right\}\cdot\left\{b_1^i,\cdots,b_{r_i}^i\right\}.\]


By this change of coordinates we can assume that $\tilde{P}_i$ has the special form 
\[
    \tilde{P}_i(\theta,\eta)=\Pi_i(\theta,\eta)\otimes w_i(\theta) w_i(\theta)^T,
\]
i.e. each $r_i\times r_i$ sub-block is a monomial multiple of the first \textit{principal} $r_i\times r_i$ sub-block $\Pi_i$ and $w_i(\theta)$ is a vector of monomials in $\theta$. Then $\tilde{f}$ can be written as 
\[
    \tilde{f}(\theta, \eta)=\sum_{i=1}^h\left<S_i(\theta),\Pi_i(\theta,\eta)\right>,
\]
where the $S_i$ are SOS matrices i.e. $S_i\in \R[\mathbf{x}]^{m_i\times m_i}$ and $\mathbf{y}^TS\textbf{y}$ 
is a sum of squares in $\R[\textbf{x},\textbf{y}]$. Note that by our construction above, the $\Pi_i$ do not depend on the choice of the polynomial $f$.

Once again we conclude the section with a road map:

\[f(\mathbf{x}) = \sum_{i= 1}^{r} (p_i(\mathbf{x}))^2 
                = \sum_{i=1}^{h}\left<Q_i,P_i(\mathbf{x})\right> 
                = \sum_{i=1}^{h}\left<Q_i,\tilde{P_i}(\theta,\eta)\right>
                = \sum_{i=1}^{h}\left<S_i(\theta),\Pi_i(\theta,\eta)\right> 
                \rightarrow f_j(\theta) = \sum_{i=1}^{h}\left<S_i(\theta),\Pi_i^j(\theta)\right> . \]

\section*{Algorithm}
Here we will present an outline of two algorithms presented in the paper
\subsection*{Algorithm I}


\textbf{Input:} Linear representation $\vartheta$ of a finite group $G$ on $\R^n$.

\begin{enumerate}
    \item Determine all real irreducible representations of $G$.
    \item Compute primary and secondary invariants $\theta_i,\eta_j$. 
    \item For each non-trivial irreducible representation compute the basis $b_1^i,\cdots, b_{r_i}^i$ of the module of equivariants.
    \item For each irreducible representation $i$ compute the corresponding matrix $\Pi_i$.
\end{enumerate}
\noindent
\textbf{Output:} Primary and secondary invariants $\theta,\eta$ and the matrices $\Pi_i$.

\subsection*{Algorithm II}

\textbf{Input: } Primary and secondary invariants $\theta,\eta$, matrices $\Pi_i$ and $f \in \R[\theta]^G$.

\begin{enumerate}
    \item Rewrite $f$ in fundamental invariants giving $\tilde{f}(\theta,\eta)$.
    \item For each irreducible representation determine $w_i(\theta)$ and thus the structure of the matrices $S_i\in \R[\theta]$. %TODO: Should probably define $w_i(\theta)$ in section 2
    \item Find a feasible solution of the semidefinite program corresponding to the constraints.
\end{enumerate}
\noindent
\textbf{Output:} SOS matrices $S_i$ providing a generalized sum of squares decomposition of $\tilde{f}$.\\



Algorithm I does the preprocessing for our problem while the second one, only having a linear representation as an input
this means that we can run the first algorithm once and then 
use the outputs for finding solution for 
all polynomials that are invariant with respect to this particular representation.


\section*{Example}
We recap an example from the paper:


Let $D_4=\left<r,s\mid r^4=s^2=(rs)^2=1\right>$ be the dihedral group of order $8$, generated by a reflection $s$ and a rotation $r$ by $\pi/2$. We let $D_4$ act on $\R[x,y]$ by 
\begin{align*}
    \vartheta(r)(x,y)&=(-y,x)\\
    \vartheta(s)(x,y)&=(y,x)
\end{align*}
Let's execute algorithm I:
\begin{enumerate}
    \item The group $G$ has five irreducible representations $(\vartheta_i)_{i=1}^5$. The trivial representation is $\vartheta_1$ and $\dim \vartheta_5=2$, while all others have dimension 1.\\
    On polynomials of degree $\leq3$, our representation decomposes as $\vartheta=2\vartheta_1\oplus\vartheta_3\oplus\vartheta_4\oplus3\vartheta_5$. A symmetry-adapted basis for this action given by   
    \begin{align*}
        v_1&=(1,x^2+y^2)&&\\
        v_2&=\emptyset&&\\
        v_3&=(xy) &   v_{51}&=(x,x^3,xy^2) \\
        v_4&=(x^2-y^2) &     v_{52}&=(y,y^3,yx^2)
    \end{align*}
    
    \item The invariant ring has the Hironaka decomposition
    \[
        \R[x,y]^G=\R[\theta_1(x,y),\theta_2(x,y)],
    \]
    where $\theta_1(x,y)=x^2+y^2$ and $\theta_2(x,y)=x^2y^2$ are the primary invariants. Secondary invariants do not appear in this case (this is always true for groups generated by reflections).\\ %TODO: I guess I mentioned the reflection group thingy here
    
    \item Bases of the modules of equivariants are given by $b_1^2=b_1^3=b_1^4=1$, while $b_1^5=\begin{pmatrix}x\\y\end{pmatrix}$ and $b_2^5=\begin{pmatrix}x^3\\y^3\end{pmatrix}$.
    
    \item Rewriting our symmetry-adapted basis in terms of $\theta$ we compute $P_i=\sum_{j=1}^{n_i} v_{ij}v_{ij}^T$ to be
    \begin{align*}
        P_1&=\begin{pmatrix}1&\theta_1\\\theta_1&\theta_1^2\end{pmatrix}&
        P_3&=\begin{pmatrix}\theta_2\end{pmatrix}\\
        P_4&=\begin{pmatrix}\theta_1^2-4\theta_2\end{pmatrix}&
        P_5&=\begin{bmatrix}\Pi_5&\theta_1\Pi_5\\\theta_1\Pi_5&\theta_1^2\Pi_5\end{bmatrix}
    \end{align*}
    where $\Pi_5=\begin{pmatrix}\theta_1&\theta_1^2-2\theta_2\\\theta_1^2-2\theta_2&\theta_1(\theta_1^2-3\theta_2)\end{pmatrix}$. In particular, $\Pi_1=1$,  $\Pi_3=P_3$ and $\Pi_4=P_4$.\\
    Note that we do not ever need to care about $i=2$ because $\vartheta_2$ does not occur in $\vartheta$. This is an accident which can only happen in low enough degrees.\\
\end{enumerate}

This concludes the preprocessing of algorithm I.\\
Note that we are able to use these calculations for every polynomial of degree at most 6 which is invariant under $G$.\\


Let 
\[
    f(x,y)=x^6+y^6-x^4y^2-y^4x^2-x^4-y^4-x^2-y^2+3x^2y^2+1
\]
and note that this is $G$-invariant, thus we can apply algorithm II:
\begin{enumerate}
    \item Rewrite $f$ in terms of $\theta_1$ and $\theta_2$:
    \[
        f(x,y)=\tilde{f}(\theta_1,\theta_2)=\theta_1^3-\theta_1^2-4\theta_1\theta_2-\theta_1+5\theta_2+1.
    \]

    \item From the results of algorithm I, we see that $w_2=w_3=w_4=1$, while $w_1=w_5=\begin{pmatrix}1\\\theta_1\end{pmatrix}$.
    
    \item By solving the SDP's, we find that $f^{SOS}=-\frac{3825}{4096}$.
\end{enumerate}

\section*{Conclusion}
Although it might seem cumbersome to find all the invariants of a representation and to work out basis for invariants of the full invariant ring of polynomials the computational time saved are quite significant. The paper showed an example how the methods described above were able to reduce a SDP problem of dimension $286\time286$ to five coupled smaller ones, of dimension $44,26,24,23$ and $5$ and another SDP problem of dimension $715\times715$ to many coupled smaller ones, of dimension $55$,$10$ ($45$ problems) and $1$ ($210$ problems) an obvious reduction in computation.


\nocite{Gatermann_2004}

\printbibliography
\end{document}